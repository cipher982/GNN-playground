{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "pass", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1606858440744_0005</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-32-23-241.ec2.internal:20888/proxy/application_1606858440744_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-32-20-90.ec2.internal:8042/node/containerlogs/container_1606858440744_0005_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import logging\n\nimport os\nimport sys\nimport json\nfrom pyspark.sql import SparkSession\n\nimport boto3\nfrom datetime import datetime\nfrom urllib.parse import urlparse\n\n# https://stackoverflow.com/questions/39235704/split-spark-dataframe-string-column-into-multiple-columns\nfrom pyspark.sql.functions import split\nfrom pyspark.sql.functions import collect_set, collect_list # collect_set eliminates duplicates, collect_list does not\n\n\nLOG = logging.getLogger(__name__)\nLOG.setLevel(logging.INFO)\n\n# TODO: log file does not get saved due to permission issues on EMR\n# formatter_file = logging.Formatter('%(filename)s:%(name)s:%(asctime)s:%(levelname)s:%(message)s')\n# file_handler = logging.FileHandler('ai_audience.log')\n# file_handler.setLevel(logging.INFO)\n# file_handler.setFormatter(formatter_file)\n\n# formatter_stream = logging.Formatter('%(levelname)s:%(message)s')\n# stream_handler = logging.StreamHandler()\n# stream_handler.setLevel(logging.INFO)\n# stream_handler.setFormatter(formatter_stream)\n\n# LOG.addHandler(file_handler)\n# LOG.addHandler(stream_handler)\n\nPATH_ROOT = 's3://zeta-dc-ml/ai-audiences/'\nPATH_COOKIE_COOKIES_RELATIONS = 's3://zeta-dcp-prod-private-tables/datacloud_cookie_cookies_relations_links_export/'\n\n\nclass CookieMapUpdate:\n\n    def __init__(self,\n                 path_root=PATH_ROOT,\n                 path_cookie_cookies_relations=PATH_COOKIE_COOKIES_RELATIONS,\n                 ):\n\n        self.path_root = path_root\n        self.path_cookie_relations_dest_root = self.path_root + 'datacloud_cookie_cookie_relations/'\n        self.path_cookie_cookie_staging = self.path_cookie_relations_dest_root + 'cookie_cookie_staging/'\n        self.path_disqus_to_sizmek = self.path_cookie_relations_dest_root + 'disqus_to_sizmek/'\n        self.path_zync_to_sizmek = self.path_cookie_relations_dest_root + 'zync_to_sizmek/'\n\n        self.path_cookie_cookies_relations = path_cookie_cookies_relations\n\n        self.list_of_all_dates_that_can_be_processed = None\n        self.date_processed_latest = None\n        self.date_processed_latest_counter = 0\n\n    @staticmethod\n    def get_list_of_all_dates_present(bucket, prefix):\n\n        s3 = boto3.client(\"s3\")\n        all_objects = s3.list_objects(Bucket=bucket,\n                                      Prefix=prefix,\n                                      Delimiter='/')\n\n        list_date_buckets = []\n        for o in all_objects.get('CommonPrefixes'):\n            list_date_buckets.append(o.get('Prefix'))\n\n        LOG.info(f'number of buckets in s3://{bucket}/{prefix} are {len(list_date_buckets)}')\n\n        list_dates_present = [el.split('/')[-2].split('=')[-1] for el in list_date_buckets]\n\n        list_dates_that_can_be_processed = []\n\n        count = 0\n        for el in list_dates_present:\n            try:\n                list_dates_that_can_be_processed.append(datetime.strptime(el, '%Y-%m-%d'))\n            except:\n                count = count + 1\n                LOG.warning(f'date partition that cannot be processed: {el}')\n\n        LOG.info(f'number of buckets that cannot be processed in s3://{bucket}/{prefix} are {count}')\n        LOG.info(f'number of buckets that can be processed in s3://{bucket}/{prefix} are {len(list_dates_that_can_be_processed)}')\n\n        list_dates_that_can_be_processed.sort()\n\n        return list_dates_that_can_be_processed\n\n    def get_list_dates_that_can_be_processed(self):\n\n        parsed_url = urlparse(self.path_cookie_cookies_relations, allow_fragments=False)\n\n        bucket = parsed_url.netloc\n        rest_of_path = parsed_url.path.lstrip('/')\n\n        self.list_of_all_dates_that_can_be_processed = self.get_list_of_all_dates_present(bucket=bucket,\n                                                                                          prefix=rest_of_path)\n\n    def get_date_processed_latest(self):\n\n        self.date_processed_latest = self.list_of_all_dates_that_can_be_processed[-1 - self.date_processed_latest_counter].strftime('%Y-%m-%d')\n        LOG.info(f'Latest Date to process is {self.date_processed_latest}')\n\n    def prep_cookie_cookie_map_table(self):\n\n        # Read from the latest, if the latest fails, read from the second to latest, else keep going back upto 10? days\n        num_read_attempts = 10\n        self.get_list_dates_that_can_be_processed()\n        for i in range(num_read_attempts):\n            try:\n                self.get_date_processed_latest()\n                df = (spark.read.option(\"inferSchema\", True)\n                      .orc(os.path.join(self.path_cookie_cookies_relations,\n                                        f'dt={self.date_processed_latest}',\n                                        '*'\n                                        )\n                           )\n                      )\n                LOG.info(f'Source read successful for date: {self.date_processed_latest}')\n                break\n            except:\n                # TODO: Alert AI-ML team as this could mean a change in table/schema\n                LOG.warning(f'Source read failed for date: {self.date_processed_latest}')\n                self.date_processed_latest_counter = self.date_processed_latest_counter + 1\n\n        if self.date_processed_latest_counter == 10:\n            LOG.error('Cookie - Cookie Mapping table has issues')\n            return None\n\n        split_col = split(df['cookie'], '::')\n        df = df.withColumn('source', split_col.getItem(0))\n        df = df.withColumn('cookie_source', split_col.getItem(1))\n\n        split_col = split(df['relation'], '::')\n        df = df.withColumn('destination', split_col.getItem(0))\n        df = df.withColumn('cookie_destination', split_col.getItem(1))\n\n        df = df.select(df['source'], df['cookie_source'], df['destination'], df['cookie_destination'], df['last_updated'])\n\n        try:\n            df.write.parquet(os.path.join(self.path_cookie_cookie_staging,\n                                          f'dt={self.date_processed_latest}'\n                                          )\n                             )\n            LOG.info(f'processed cookie_cookie_mappings with latest date processed: {self.date_processed_latest}')\n        except:\n            LOG.warning(f'data of cookie_cookie_mappings for date {self.date_processed_latest} might already exist')\n\n    def prep_disqus_sizmek_map_table(self):\n\n        bucket = self.path_cookie_cookie_staging.split('/')[2]\n        rest_of_path = os.path.join(*(self.path_cookie_cookie_staging.split('/')[3:]))\n\n        temp_list_of_dates = self.get_list_of_all_dates_present(bucket=bucket,\n                                                                prefix=rest_of_path)\n        date_processed_latest_ = temp_list_of_dates[-1].strftime('%Y-%m-%d')\n\n        df = (spark.read.option(\"inferSchema\", True)\n              .parquet(os.path.join(self.path_cookie_cookie_staging,\n                                    f'dt={date_processed_latest_}',\n                                    '*parquet')\n                       )\n              )\n\n        df_disqus_to_sizmek = df.filter((df.source == 'disqus') & (df.destination == 'sizmek'))\n\n        df_disqus_to_sizmek = df_disqus_to_sizmek.selectExpr('cookie_source as cookie_disqus',\n                                                             'cookie_destination as cookie_dest')\n        df_disqus_to_sizmek = df_disqus_to_sizmek.drop_duplicates()\n\n        df_disqus_to_sizmek = (df_disqus_to_sizmek\n                               .groupBy('cookie_disqus')\n                               .agg(collect_set('cookie_dest').alias('cookie_sizmek'))\n                               )\n        try:\n            df_disqus_to_sizmek.write.parquet(os.path.join(self.path_disqus_to_sizmek,\n                                                           f'dt={date_processed_latest_}'\n                                                           )\n                                              )\n            LOG.info(f'processed disqus_to_sizmek_cookie_mappings with latest date processed: {date_processed_latest_}')\n        except:\n            LOG.warning(f'data of disqus_to_sizmek for date {date_processed_latest_} might already exist')\n\n    def prep_zync_sizmek_map_table(self):\n\n        bucket = self.path_cookie_cookie_staging.split('/')[2]\n        rest_of_path = os.path.join(*(self.path_cookie_cookie_staging.split('/')[3:]))\n\n        temp_list_of_dates = self.get_list_of_all_dates_present(bucket=bucket,\n                                                                prefix=rest_of_path)\n        date_processed_latest_ = temp_list_of_dates[-1].strftime('%Y-%m-%d')\n\n        df = (spark.read.option(\"inferSchema\", True)\n              .parquet(os.path.join(self.path_cookie_cookie_staging,\n                                    f'dt={date_processed_latest_}',\n                                    '*.parquet')\n                       )\n              )\n\n        df_zync_to_sizmek = df.filter((df.source == 'zync') & (df.destination == 'sizmek'))\n\n        df_zync_to_sizmek = df_zync_to_sizmek.selectExpr('cookie_source as cookie_zync',\n                                                         'cookie_destination as cookie_dest')\n        df_zync_to_sizmek = df_zync_to_sizmek.drop_duplicates()\n\n        df_zync_to_sizmek = (df_zync_to_sizmek\n                             .groupBy('cookie_zync')\n                             .agg(collect_set('cookie_dest').alias('cookie_sizmek'))\n                             )\n\n        try:\n            df_zync_to_sizmek.write.parquet(os.path.join(self.path_zync_to_sizmek,\n                                                         f'dt={date_processed_latest_}'\n                                                         )\n                                            )\n            LOG.info(f'processed zync_to_sizmek_cookie_mappings with latest date processed: {date_processed_latest_}')\n        except:\n            LOG.warning(f'data of zync_to_sizmek for date {date_processed_latest_} might already exist')\n\n    def run_step(self):\n        try:\n            self.prep_cookie_cookie_map_table()\n        except:\n            # TODO: Alert AI-ML team about a potential change in table/schema\n            LOG.warning('difficulty reading cookie cookie mapping table')\n        self.prep_disqus_sizmek_map_table()\n        self.prep_zync_sizmek_map_table()\n        LOG.info('finished step CookieMapUpdate')", "execution_count": 5, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "path_root = 's3://zeta-dc-ml/ai-audiences/'\n\nconfig_step_z12 = {\n        'path_root': path_root,\n        'path_cookie_cookies_relations': 's3://zeta-dcp-prod-private-tables/datacloud_cookie_cookies_relations_links_export/',\n}\n\n\nstep_cookie_map_update = CookieMapUpdate(path_root=config_step_z12['path_root'],\n                                         path_cookie_cookies_relations=config_step_z12['path_cookie_cookies_relations'],\n                                        )\n", "execution_count": 6, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "step_cookie_map_update.prep_disqus_sizmek_map_table()", "execution_count": 7, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "step_cookie_map_update.prep_zync_sizmek_map_table()", "execution_count": 8, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "path_sizmek = 's3://zeta-dc-ml/ai-audiences/modeling/dt=2020-11-17/z21_user_item_interactions_sizmek/'\npath_zync = 's3://zeta-dc-ml/ai-audiences/modeling/dt=2020-11-17/z22_user_item_interactions_zync/'\npath_disqus = 's3://zeta-dc-ml/ai-audiences/modeling/dt=2020-11-17/z23_user_item_interactions_disqus/'", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df = spark.read.option(\"inferSchema\", True).parquet(path_sizmek + '*.parquet')", "execution_count": 4, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.printSchema()", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "root\n |-- user_id: string (nullable = true)\n |-- zcodes: string (nullable = true)\n |-- count: long (nullable = true)", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df2 = spark.read.option(\"inferSchema\", True).parquet(path_zync + '*.parquet')\ndf2.printSchema()", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "root\n |-- user_id: string (nullable = true)\n |-- zcodes: string (nullable = true)\n |-- count: long (nullable = true)", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df3 = spark.read.option(\"inferSchema\", True).parquet(path_disqus + '*.parquet')\ndf3.printSchema()", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "root\n |-- user_id: string (nullable = true)\n |-- zcodes: string (nullable = true)\n |-- count: long (nullable = true)", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pyspark3kernel", "display_name": "PySpark3", "language": ""}, "language_info": {"name": "pyspark3", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 3}, "pygments_lexer": "python3"}}, "nbformat": 4, "nbformat_minor": 2}